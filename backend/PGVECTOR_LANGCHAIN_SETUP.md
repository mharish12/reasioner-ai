# pgvector + LangChain Integration Setup

This guide explains how to set up and use pgvector with LangChain for RAG (Retrieval Augmented Generation) to reduce hallucinations and provide accurate responses.

## Features

- ✅ **pgvector Integration**: Store and query vector embeddings directly in PostgreSQL
- ✅ **LangChain RAG**: Use LangChain for retrieval-augmented generation
- ✅ **Multiple LLM Support**:
  - Ollama (free, local)
  - OpenAI (paid, cloud)
  - HuggingFace (free, local)
- ✅ **Reduced Hallucinations**: Responses are grounded in training data
- ✅ **Efficient Similarity Search**: Fast vector similarity queries using pgvector

## Prerequisites

1. **PostgreSQL with pgvector extension**

   - PostgreSQL 11 or higher
   - pgvector extension installed

2. **Python dependencies** (already in requirements.txt):
   - `langchain>=0.1.0`
   - `langchain-community>=0.0.20`
   - `langchain-postgres>=0.0.3`
   - `langchain-ollama>=0.1.0` (for Ollama)
   - `langchain-openai>=0.0.5` (for OpenAI)
   - `pgvector>=0.2.3`
   - `sentence-transformers>=2.2.2`

## Installation Steps

### 1. Install Python Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### 2. Set Up PostgreSQL with pgvector

#### Option A: Using Docker (Recommended)

```bash
docker run -d \
  --name postgres-pgvector \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=ai_platform \
  -p 5432:5432 \
  pgvector/pgvector:pg16
```

#### Option B: Install pgvector Extension Manually

```sql
-- Connect to your PostgreSQL database
CREATE EXTENSION IF NOT EXISTS vector;
```

### 3. Run Database Migration

Execute the pgvector migration:

```bash
# Using psql
psql -U postgres -d ai_platform -f migrations/enable_pgvector.sql

# Or using Python
python -c "
from config.database import engine
from sqlalchemy import text
with engine.connect() as conn:
    conn.execute(text('CREATE EXTENSION IF NOT EXISTS vector'))
    conn.commit()
"
```

### 4. Configure LLM (Choose One)

#### Option A: Ollama (Free, Recommended)

1. Install Ollama: https://ollama.ai/
2. Download a model:
   ```bash
   ollama pull llama2
   # or
   ollama pull mistral
   ```
3. No API key needed - runs locally!

#### Option B: OpenAI (Paid)

1. Get API key from https://platform.openai.com/
2. Set environment variable:
   ```bash
   export OPENAI_API_KEY=your_api_key_here
   ```

#### Option C: HuggingFace (Free)

1. Install transformers (already in requirements.txt)
2. Models download automatically on first use
3. Note: Can be slower, requires more memory

## Usage

### Training a LangChain RAG Model

Use the API endpoint with `model_type: "langchain_rag"`:

```bash
curl -X POST "http://localhost:8000/api/train/" \
  -H "Content-Type: multipart/form-data" \
  -F "agent_id=1" \
  -F "model_type=langchain_rag" \
  -F "model_name=my_rag_model" \
  -F 'parameters={"llm_type": "ollama", "llm_config": {"model_name": "llama2"}, "embedding_model": "all-MiniLM-L6-v2", "top_k": 3}' \
  -F "files=@document1.pdf" \
  -F "files=@document2.txt"
```

#### Parameters Explained

```json
{
  "llm_type": "ollama", // Options: "ollama", "openai", "huggingface"
  "llm_config": {
    "model_name": "llama2", // Model name for Ollama
    "temperature": 0.7, // Creativity (0.0-1.0)
    "base_url": "http://localhost:11434" // Ollama base URL
  },
  "embedding_model": "all-MiniLM-L6-v2", // Embedding model
  "top_k": 3 // Number of documents to retrieve
}
```

For OpenAI:

```json
{
  "llm_type": "openai",
  "llm_config": {
    "model_name": "gpt-3.5-turbo",
    "temperature": 0.7
  }
}
```

### Querying the Model

```bash
curl -X POST "http://localhost:8000/api/query/" \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": 1,
    "query_text": "What is the main topic of the documents?"
  }'
```

The response will be:

- Based on retrieved relevant documents from pgvector
- Generated by the LLM using LangChain RAG
- Grounded in actual training data (reduces hallucinations)

## How It Works

1. **Training Phase**:

   - Documents are processed and split into chunks
   - Embeddings are generated using sentence-transformers
   - Embeddings are stored in PostgreSQL using pgvector
   - Vector index is created for fast similarity search

2. **Query Phase**:
   - User query is converted to embedding
   - Similar documents are retrieved using pgvector cosine similarity
   - Top K most relevant documents are passed to LLM
   - LLM generates response based on retrieved context
   - Response is grounded in actual documents (prevents hallucinations)

## Architecture

```
Training Documents
    ↓
Document Processing
    ↓
Embedding Generation (sentence-transformers)
    ↓
Storage in PostgreSQL (pgvector)
    ↓
Vector Index Creation

Query
    ↓
Query Embedding
    ↓
pgvector Similarity Search
    ↓
Retrieve Top K Documents
    ↓
LangChain RAG Chain
    ↓
LLM (Ollama/OpenAI/HuggingFace)
    ↓
Grounded Response
```

## Troubleshooting

### pgvector Extension Not Found

```sql
-- Check if extension is installed
SELECT * FROM pg_extension WHERE extname = 'vector';

-- If not installed, run:
CREATE EXTENSION vector;
```

### Ollama Connection Error

- Make sure Ollama is running: `ollama serve`
- Check Ollama is accessible: `curl http://localhost:11434/api/tags`
- Verify model is downloaded: `ollama list`

### OpenAI API Error

- Verify API key: `echo $OPENAI_API_KEY`
- Check API key is valid and has credits
- Ensure you have access to the model (e.g., gpt-3.5-turbo)

### Memory Issues with HuggingFace

- Use smaller models (e.g., `microsoft/DialoGPT-small`)
- Increase available RAM or use GPU
- Consider using Ollama instead (more efficient)

## Performance Tips

1. **Embedding Model**: Use `all-MiniLM-L6-v2` (384 dims) for faster processing, or `all-mpnet-base-v2` (768 dims) for better quality
2. **Vector Index**: The migration creates an IVFFlat index for faster searches
3. **Top K**: Lower `top_k` values (3-5) are usually sufficient and faster
4. **Batch Processing**: For large document sets, process in batches

## Migration from FAISS RAG

If you're using the old `rag` model type (FAISS-based):

1. Train a new `langchain_rag` model with the same documents
2. Update your client code to use the new model_id
3. Old `rag` models will continue to work but won't benefit from LangChain

## Example Python Client

```python
import requests

# Train model
response = requests.post(
    "http://localhost:8000/api/train/",
    files={"files": open("document.pdf", "rb")},
    data={
        "agent_id": 1,
        "model_type": "langchain_rag",
        "model_name": "my_model",
        "parameters": '{"llm_type": "ollama", "llm_config": {"model_name": "llama2"}}'
    }
)
model_id = response.json()["id"]

# Query model
response = requests.post(
    "http://localhost:8000/api/query/",
    json={
        "model_id": model_id,
        "query_text": "Summarize the document"
    }
)
print(response.json()["response"])
```

## Next Steps

- Fine-tune embedding models for your domain
- Adjust prompt templates for better responses
- Monitor query performance and optimize vector index
- Add more context in the RAG chain for better accuracy
